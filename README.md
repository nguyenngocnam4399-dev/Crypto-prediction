# ğŸš€ Deterministic Quantitative Trading Research Platform  
### Production-Grade Crypto Data Pipeline with Structured Edge Validation  

### Data Engineering Capstone Project  
**Author:** Nguyá»…n Ngá»c Nam  
**Mentor:** Pháº¡m Long VÃ¢n - Data Manager

**Location:** Ho Chi Minh City, Vietnam â€” 2025  

---

# 1ï¸âƒ£ Introduction

This project designs a structured, scalable, and empirically testable quantitative trading research framework. It standardizes the full lifecycle of a trading signalâ€”from real-time market ingestion, technical indicator computation, and metric abstraction, to deterministic prediction and independent confirmationâ€”while strictly separating processing layers to prevent data leakage.

Instead of relying on opaque machine learning models, the system implements a weighted, metric-driven scoring engine based on the concept of **edge**, allowing transparent evaluation of directional dominance between buyers and sellers. The architecture follows a fact-driven Data Warehouse design with explicit grain definition, idempotent ETL processes, and full signal traceability. Structural pattern mining (FP-Growth) is applied to validated trades to assess edge sustainability. The framework prioritizes transparency, reproducibility, and experimental rigor over short-term optimization.

---

# 2ï¸âƒ£ System Outputs & User Value

This platform delivers multiple layers of value to quantitative researchers and trading system developers:

## 1. Explainable Trading Signals
- Deterministic BUY / SELL decisions  
- Edge and confidence scoring  
- Full transparency of contributing metrics  
- No black-box logic  

## 2. Controlled Performance Evaluation
- Leakage-safe confirmation framework  
- Adaptive TP/SL risk modeling  
- Expectancy, drawdown, rolling stability analysis  
- Equity curve simulation  

## 3. Structural Market Insights
- Regime-dependent performance segmentation  
- FP-Growth structural pattern mining  
- Lift-based edge validation  
- Identification of recurring market conditions  

## 4. Scalable Research Infrastructure
- Warehouse-driven experimentation  
- Configurable metric logic  
- DAG-based orchestration  
- Fully reproducible signal lifecycle  

The system transforms raw market data into structured trading intelligence while maintaining transparency, scalability, and experimental control.

---

# 3ï¸âƒ£ System Architecture

![System Architecture](images/System_Architecture.png)

The system follows a layered architecture separating ingestion, transformation, signal modeling, orchestration, validation, and analytics.

---

## 1. Data Ingestion

- Kafka streams real-time OHLCV market data  
- Spark Streaming processes and normalizes records  
- Clean data stored in `fact_kline`  

---

## 2. Indicator Computation

- Atomic indicators computed via Spark  
- Stored independently in `fact_indicator`  
- Partitioned by symbol and interval  

---

## 3. Metric Abstraction

- Trading conditions defined in `dim_metric`  
- Evaluated into `fact_metric_value`  
- Threshold, trend, cross, volatility logic  

---

## 4. Prediction Engine

- Independent BUY / SELL scoring  
- Edge and confidence calculation  
- Stored in `fact_prediction`  

---

## 5. Backtesting & Confirmation

- Adaptive TP/SL within controlled lookahead  
- Stored in `fact_prediction_result`  
- Strict leakage prevention  

---

## 6. Orchestration Layer (Apache Airflow)

The entire workflow is coordinated using **Apache Airflow DAGs** running in a Linux environment.

- Dependency-controlled Spark job execution  
- Scheduled metric, prediction, and confirmation tasks  
- Retry and failure handling  
- Modular DAG structure  
- CeleryExecutor for distributed execution  

Airflow ensures reproducible, production-ready orchestration across all pipeline layers.

---

## 7. Analytics & Pattern Mining

- Equity curve and expectancy metrics  
- Regime-based performance analysis  
- FP-Growth structural mining  
- Flask API for analytics endpoints  

---

# 4ï¸âƒ£ Data Warehouse Design

![Warehouse ERD](images/warehouse_schema.png)

Fact-driven layered model with explicit grain definition.

## Core Dimensions

- `dim_symbol`
- `dim_interval`
- `dim_indicator_type`
- `dim_metric`

## Fact Layers

| Table                    | Grain                                      | Role |
|--------------------------|--------------------------------------------|------|
| `fact_kline`             | (symbol, interval, close_time)             | Market data |
| `fact_indicator`         | (symbol, interval, indicator, timestamp)   | Atomic signals |
| `fact_metric_value`      | (symbol, interval, metric, calculating_at) | Logical conditions |
| `fact_prediction`        | (symbol, interval, predicting_at)          | Trading hypothesis |
| `fact_prediction_result` | (prediction_id)                            | Realized outcome |

---

# 5ï¸âƒ£ Indicator Engineering

Indicators stored at atomic grain:

(symbol_id, interval_id, indicator_type, timestamp)

- Spark window-based computation  
- Partitioned distributed processing  
- Fully recomputable from raw kline  

---

# 6ï¸âƒ£ Metric Abstraction Layer

Config-driven trading logic defined in database.

- Anchor indicator  
- Threshold range  
- Direction rule  
- Window size  
- Weight  

Evaluated into `fact_metric_value`.

---

# 7ï¸âƒ£ Prediction Engine

buy_score  = Î£(weighted BUY metrics)  
sell_score = Î£(weighted SELL metrics)  

edge = |buy_score âˆ’ sell_score|  
confidence = max(score) / MAX_SCORE  

Stored in `fact_prediction`.

---

# 8ï¸âƒ£ Backtesting & Confirmation Framework

- Controlled lookahead window  
- Adaptive TP/SL  
- Idempotent result writing  
- Strict leakage prevention  

---

# 9ï¸âƒ£ Analytics & Performance Evaluation

- Win Rate  
- Expectancy  
- Rolling stability  
- Equity curve  
- Drawdown  
- FP-Growth structural validation  

---

# ğŸ”Ÿ Tech Stack & Engineering Practices

## Tech Stack

- Python  
- PySpark  
- Apache Kafka  
- Apache Airflow (CeleryExecutor)  
- Spark ML (FPGrowth)  
- MySQL 8  
- Flask  
- NumPy / Pandas  

## Deployment Environment

The system is deployed and executed on a **Linux-based environment** with:

- Apache Kafka running as service  
- Apache Spark cluster  
- Apache Airflow scheduler + workers  
- MySQL database server  
- Flask API service  

Services are configured and managed directly at the OS level without containerization.

---

## Engineering Practices

- Layered architecture  
- Explicit grain control  
- Fact-driven warehouse modeling  
- Idempotent ETL  
- UTC normalization  
- Distributed Spark computation  
- Config-driven strategy logic  
- Strict prediction/confirmation separation  
- DAG-based workflow orchestration  
- Linux service-level deployment  

---

# 11ï¸âƒ£ Conclusion

This project establishes a deterministic quantitative research infrastructure grounded in structured data modeling and strict leakage control.

By decoupling ingestion, signal construction, confirmation, and evaluation into independent layers, the system enforces reproducibility, auditability, and disciplined experimentation. Every stage of the signal lifecycle is persisted at explicit grain within a fact-driven warehouse, reducing ambiguity and preventing implicit assumptions.

The framework is designed as a foundation rather than a single strategy implementation. Its architecture allows controlled expansion toward portfolio-level allocation models, transaction cost integration, regime-adaptive signal weighting, and hybrid deterministicâ€“statistical extensions, while preserving transparency and structural integrity.

The primary objective is not short-term optimization, but the construction of a research environment where edge can be measured, validated, and stress-tested under controlled conditions.
# ğŸ“‚ Project Structure

---

```text
crypto-quant-platform/
â”‚
â”œâ”€â”€ dags/                         # Airflow DAG definitions
â”‚   â”œâ”€â”€ crypto_pipeline_dag.py
â”‚   â”œâ”€â”€ news_pipeline_dag.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer_prices.py
â”‚   â”œâ”€â”€ producer_news.py
â”‚   â”œâ”€â”€ consumer_prices.py
â”‚   â””â”€â”€ consumer_news.py
â”‚
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ staging_to_fact.py
â”‚   â”œâ”€â”€ indicator_job.py
â”‚   â”œâ”€â”€ metric_job.py
â”‚   â”œâ”€â”€ prediction_engine.py
â”‚   â”œâ”€â”€ backtest_engine.py
â”‚   â””â”€â”€ fp_growth_job.py
â”‚
â”œâ”€â”€ indicators/
â”‚   â”œâ”€â”€ momentum.py
â”‚   â”œâ”€â”€ trend_filter.py
â”‚   â”œâ”€â”€ trend_strength.py
â”‚   â”œâ”€â”€ volatility.py
â”‚   â””â”€â”€ volume.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ dim_tables.sql
â”‚   â”œâ”€â”€ fact_tables.sql
â”‚   â””â”€â”€ metric_definitions.sql
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


